\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\AC@reset@newl@bel
\citation{bilby}
\bibdata{mainNotes,refs}
\bibcite{bilby}{{1}{2019}{{Ashton\ \emph  {et~al.}}}{{Ashton, Huebner, Lasky, {Colm Talbot}, Ackley, {Sylvia Biscoveanu}, Chu, Divarkala, Easter, {Boris Goncharov}, {Francisco Hernandez Vivanco}, Harms, Lower, {Grant D. Meadors}, Melchor, Payne, Pitkin, Powell, Sarin, {Rory J. E. Smith},\ and\ Thrane}}}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{Explainable Deep-learning: Monte Carlo methods for Gravitational-Wave Inference}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{section*.1}\protected@file@percent }
\newacro{GW}[GW]{Gravitational wave}
\newacro{BBH}[BBH]{binary black hole}
\newacro{EM}[EM]{electromagnetic}
\newacro{CBC}[CBC]{compact binary coalescence}
\newacro{BNS}[BNS]{binary neutron star}
\newacro{NSBH}[NSBH]{neutron star black hole}
\newacro{PSD}[PSD]{power spectral density}
\newacro{ELBO}[ELBO]{evidence lower bound}
\newacro{LIGO}[LIGO]{advanced Laser Interferometer Gravitational wave Observatory}
\newacro{CVAE}[CVAE]{conditional variational autoencoder}
\newacro{KL}[KL]{Kullback--Leibler}
\newacro{GPU}[GPU]{graphics processing unit}
\newacro{LVC}[LVC]{LIGO-Virgo Collaboration}
\newacro{PP}[p-p]{probability-probability}
\newacro{SNR}[SNR]{signal-to-noise ratio}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.3}\protected@file@percent }
\newlabel{intro}{{I}{1}{}{section*.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}{\sc  VItamin}\xspace  : User-Friendly Inference}{1}{section*.4}\protected@file@percent }
\newlabel{vit}{{I\tmspace  +\thinmuskip {.1667em}A}{1}{}{section*.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Monte Carlo Framework}{1}{section*.5}\protected@file@percent }
\newlabel{theory:monte}{{I\tmspace  +\thinmuskip {.1667em}B}{1}{}{section*.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}SIR Framework}{1}{section*.6}\protected@file@percent }
\newlabel{theory:sir}{{I\tmspace  +\thinmuskip {.1667em}C}{1}{}{section*.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Methodology}{1}{section*.7}\protected@file@percent }
\newlabel{methods}{{II}{1}{}{section*.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Model Training}{1}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Likelihood Estimates}{1}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Importance Resampling}{1}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Results}{1}{section*.11}\protected@file@percent }
\newlabel{results}{{III}{1}{}{section*.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Self-consistency}{1}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Reproducibility}{1}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Importance Resampling}{1}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Conclusions}{1}{section*.15}\protected@file@percent }
\newlabel{conc}{{IV}{1}{}{section*.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Acknowledgements}{1}{section*.16}\protected@file@percent }
\bibstyle{apsrev4-1}
\citation{REVTEX41Control}
\citation{apsrev41Control}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Example of how a normalising flow trained on a set of live points can produce samples within current iso-likelihood contour for simple two-dimensional parameter space. \textbf  {Top:} example of training samples in the physical space $$ and learned mapping to the latent space $$ with the iso-likelihood contour for the current \textit  {worst point} shown in orange. \textbf  {Middle:} samples drawn from a truncated Guassian within the iso-likelihood contour in $$ and mapped to $$ using the inverse mapping. \textbf  {Bottom:} pool of accepted samples after applying rejection sampling until 1000 points are obtained shown in both $$ and $$.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:vit_flow}{{1}{2}{Example of how a normalising flow trained on a set of live points can produce samples within current iso-likelihood contour for simple two-dimensional parameter space. \textbf {Top:} example of training samples in the physical space $\physical $ and learned mapping to the latent space $\latent $ with the iso-likelihood contour for the current \textit {worst point} shown in orange. \textbf {Middle:} samples drawn from a truncated Guassian within the iso-likelihood contour in $\latent $ and mapped to $\physical $ using the inverse mapping. \textbf {Bottom:} pool of accepted samples after applying rejection sampling until 1000 points are obtained shown in both $\latent $ and $\physical $}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{2}{section*.17}\protected@file@percent }
\newlabel{LastBibItem}{{1}{2}{}{section*.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Example of how a normalising flow trained on a set of live points can produce samples within current iso-likelihood contour for simple two-dimensional parameter space. \textbf  {Top:} example of training samples in the physical space $$ and learned mapping to the latent space $$ with the iso-likelihood contour for the current \textit  {worst point} shown in orange. \textbf  {Middle:} samples drawn from a truncated Guassian within the iso-likelihood contour in $$ and mapped to $$ using the inverse mapping. \textbf  {Bottom:} pool of accepted samples after applying rejection sampling until 1000 points are obtained shown in both $$ and $$.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:learning_contours}{{2}{3}{Example of how a normalising flow trained on a set of live points can produce samples within current iso-likelihood contour for simple two-dimensional parameter space. \textbf {Top:} example of training samples in the physical space $\physical $ and learned mapping to the latent space $\latent $ with the iso-likelihood contour for the current \textit {worst point} shown in orange. \textbf {Middle:} samples drawn from a truncated Guassian within the iso-likelihood contour in $\latent $ and mapped to $\physical $ using the inverse mapping. \textbf {Bottom:} pool of accepted samples after applying rejection sampling until 1000 points are obtained shown in both $\latent $ and $\physical $}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Probability-probability (P-P) plot showing the confidence interval versus the fraction of the events within that confidence interval for the posterior distributions obtained using our analysis {\sc  Nessai}\xspace  for 128 simulated compact binary coalescence signals produced with {\sc  Bilby}\xspace  and {\sc  bilby\_pipe}\xspace  . The 1-, 2- and 3-$\sigma $ confidence intervals are indicated by the shaded regions and $p$-values are shown for each of the parameters and the combined $p$-value is also shown.}}{3}{figure.3}\protected@file@percent }
\newlabel{fig:vit_train_corner}{{3}{3}{Probability-probability (P-P) plot showing the confidence interval versus the fraction of the events within that confidence interval for the posterior distributions obtained using our analysis \nessai for 128 simulated compact binary coalescence signals produced with \bilby and \bilbypipe . The 1-, 2- and 3-$\sigma $ confidence intervals are indicated by the shaded regions and $p$-values are shown for each of the parameters and the combined $p$-value is also shown}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Diagram of a normalising flow $f(x)$ composed of four coupling transforms which maps an \ndimensional {n} input vector x to an \ndimensional {n} latent vector z. Each transform splits $x$ in two $[x_{1:m}, x_{m+1:n}]$ and updates one part conditioned on the other. In the first and third transforms $x_{1:m}$ is used as the input to a neural network (NN) which then produces the scale $s$ and translation $t$ vectors of length $m$. The element-wise product ($\odot $) is then computed between $x_{1:m}$ and $\exp (s)$ followed by the sum of the output and $t$. This is shown in the left transform. In the second and fourth transforms $x_{1:m}$ is updated conditioned on $x_{m+1:n}$ as shown in the right transform. \textcolor {red} {In my caption, link to previous sections and equations}}}{4}{figure.4}\protected@file@percent }
\newlabel{fig:monte_flow}{{4}{4}{Diagram of a normalising flow $f(x)$ composed of four coupling transforms which maps an \protect \ndimensional {n} input vector x to an \protect \ndimensional {n} latent vector z. Each transform splits $x$ in two $[x_{1:m}, x_{m+1:n}]$ and updates one part conditioned on the other. In the first and third transforms $x_{1:m}$ is used as the input to a neural network (NN) which then produces the scale $s$ and translation $t$ vectors of length $m$. The element-wise product ($\odot $) is then computed between $x_{1:m}$ and $\exp (s)$ followed by the sum of the output and $t$. This is shown in the left transform. In the second and fourth transforms $x_{1:m}$ is updated conditioned on $x_{m+1:n}$ as shown in the right transform. \textcolor {red} {In my caption, link to previous sections and equations}}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Probability-probability (P-P) plot showing the confidence interval versus the fraction of the events within that confidence interval for the posterior distributions obtained using our analysis {\sc  Nessai}\xspace  for 128 simulated compact binary coalescence signals produced with {\sc  Bilby}\xspace  and {\sc  bilby\_pipe}\xspace  . The 1-, 2- and 3-$\sigma $ confidence intervals are indicated by the shaded regions and $p$-values are shown for each of the parameters and the combined $p$-value is also shown.}}{5}{figure.5}\protected@file@percent }
\newlabel{fig:sel_consist}{{5}{5}{Probability-probability (P-P) plot showing the confidence interval versus the fraction of the events within that confidence interval for the posterior distributions obtained using our analysis \nessai for 128 simulated compact binary coalescence signals produced with \bilby and \bilbypipe . The 1-, 2- and 3-$\sigma $ confidence intervals are indicated by the shaded regions and $p$-values are shown for each of the parameters and the combined $p$-value is also shown}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Example of how a normalising flow trained on a set of live points can produce samples within current iso-likelihood contour for simple two-dimensional parameter space. \textbf  {Top:} example of training samples in the physical space $$ and learned mapping to the latent space $$ with the iso-likelihood contour for the current \textit  {worst point} shown in orange. \textbf  {Middle:} samples drawn from a truncated Guassian within the iso-likelihood contour in $$ and mapped to $$ using the inverse mapping. \textbf  {Bottom:} pool of accepted samples after applying rejection sampling until 1000 points are obtained shown in both $$ and $$.}}{5}{figure.6}\protected@file@percent }
\newlabel{fig:hists}{{6}{5}{Example of how a normalising flow trained on a set of live points can produce samples within current iso-likelihood contour for simple two-dimensional parameter space. \textbf {Top:} example of training samples in the physical space $\physical $ and learned mapping to the latent space $\latent $ with the iso-likelihood contour for the current \textit {worst point} shown in orange. \textbf {Middle:} samples drawn from a truncated Guassian within the iso-likelihood contour in $\latent $ and mapped to $\physical $ using the inverse mapping. \textbf {Bottom:} pool of accepted samples after applying rejection sampling until 1000 points are obtained shown in both $\latent $ and $\physical $}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Probability-probability (P-P) plot showing the confidence interval versus the fraction of the events within that confidence interval for the posterior distributions obtained using our analysis {\sc  Nessai}\xspace  for 128 simulated compact binary coalescence signals produced with {\sc  Bilby}\xspace  and {\sc  bilby\_pipe}\xspace  . The 1-, 2- and 3-$\sigma $ confidence intervals are indicated by the shaded regions and $p$-values are shown for each of the parameters and the combined $p$-value is also shown.}}{6}{figure.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{6}{figure.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{6}{figure.7}\protected@file@percent }
\newlabel{fig:scatter}{{7}{6}{Probability-probability (P-P) plot showing the confidence interval versus the fraction of the events within that confidence interval for the posterior distributions obtained using our analysis \nessai for 128 simulated compact binary coalescence signals produced with \bilby and \bilbypipe . The 1-, 2- and 3-$\sigma $ confidence intervals are indicated by the shaded regions and $p$-values are shown for each of the parameters and the combined $p$-value is also shown}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Corner plot comparing the posterior distributions produced with {\sc  dynesty}\xspace  (blue) and our sampler {\sc  Nessai}\xspace  (red). The phase is marginalised and remaining 14 parameters are shown, see {app:priors} for details on the parameters. The respective 16\% and 84\% quantiles are also shown in the \ndimensional {1} marginalised posteriors.}}{7}{figure.8}\protected@file@percent }
\newlabel{fig:final_corner}{{8}{7}{Corner plot comparing the posterior distributions produced with \dynesty (blue) and our sampler \nessai (red). The phase is marginalised and remaining 14 parameters are shown, see \cref {app:priors} for details on the parameters. The respective 16\% and 84\% quantiles are also shown in the \protect \ndimensional {1} marginalised posteriors}{figure.8}{}}
\newlabel{LastPage}{{}{7}{}{}{}}
